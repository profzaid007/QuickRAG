{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e3516c-36f1-4d99-a9b9-feaadb89e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "from groq import Groq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566f6326-b97b-4443-b5f0-06e74427de5b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e710f5d-07f8-4c60-95c8-f6c89403a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CYAN = '\\033[96m'\n",
    "YELLOW = '\\033[93m'\n",
    "PINK= '\\033[95m'\n",
    "NEON_GREEN='\\033[92m'\n",
    "RESET_COLOUR='\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f73149-0cb4-499e-8e6a-913acd80bf90",
   "metadata": {},
   "source": [
    "ollama Api Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45aead12-6dcf-4040-a44a-cd881589047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(\n",
    "    api_key=\"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\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e207166d-eea3-462b-b1c8-466685f98c89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for each line in the vault:\n",
      "tensor([[-0.1010,  0.0252,  0.0621,  ..., -0.0056, -0.0635, -0.0224],\n",
      "        [-0.1188,  0.0483, -0.0025,  ...,  0.1264,  0.0465, -0.0157],\n",
      "        [-0.0105, -0.0493, -0.0672,  ..., -0.0331, -0.1459,  0.0106],\n",
      "        [-0.1188,  0.0483, -0.0025,  ...,  0.1264,  0.0465, -0.0157],\n",
      "        [ 0.0334,  0.0579,  0.0395,  ...,  0.0101, -0.0631, -0.0058],\n",
      "        [-0.1188,  0.0483, -0.0025,  ...,  0.1264,  0.0465, -0.0157]])\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "CYAN = '\\033[96m'\n",
    "YELLOW = '\\033[93m'\n",
    "PINK = '\\033[95m'\n",
    "NEON_GREEN = '\\033[92m'\n",
    "RESET_COLOUR = '\\033[0m'\n",
    "\n",
    "client = Groq(\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "def open_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as infile:\n",
    "        return infile.read()\n",
    "\n",
    "def get_relevant_context(user_input, vault_embeddings, vault_content, model, top_k=3):\n",
    "    if vault_embeddings.nelement() == 0:\n",
    "        return []\n",
    "    input_embedding = model.encode([user_input])\n",
    "    cos_scores = util.cos_sim(input_embedding, vault_embeddings)[0]\n",
    "    top_k = min(top_k, len(cos_scores))\n",
    "    top_indices = torch.topk(cos_scores, k=top_k)[1].tolist()\n",
    "    relevant_context = [vault_content[idx].strip() for idx in top_indices]\n",
    "    return relevant_context\n",
    "\n",
    "def groq_chat(user_input, system_message, vault_embeddings, vault_content, model):\n",
    "    relevant_context = get_relevant_context(user_input, vault_embeddings, vault_content, model)\n",
    "    if relevant_context:\n",
    "        context_str = \"\\n\".join(relevant_context)\n",
    "        print(\"Context from RAG: \\n\\n\" + CYAN + context_str + RESET_COLOUR)\n",
    "    else:\n",
    "        print(CYAN + \"No relevant context found.\" + RESET_COLOUR)\n",
    "\n",
    "    user_input_with_context = user_input\n",
    "    if relevant_context:\n",
    "        user_input_with_context = context_str + \"\\n\\n\" + user_input\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_input_with_context}\n",
    "    ]\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama3-8b-8192\"\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "vault_content = []\n",
    "if os.path.exists(\"./dataset.txt\"):\n",
    "    with open(\"./dataset.txt\", \"r\", encoding='utf-8') as vault_file:\n",
    "        vault_content = vault_file.readlines()\n",
    "\n",
    "vault_embeddings = model.encode(vault_content) if vault_content else []\n",
    "vault_embeddings_tensor = torch.tensor(vault_embeddings)\n",
    "print(\"Embeddings for each line in the vault:\")\n",
    "print(vault_embeddings_tensor)\n",
    "\n",
    "user_input = input(YELLOW + \"Ask a question about your documents: \" + RESET_COLOUR)\n",
    "system_message = \"You are a helpful assistant that is an expert at extracting the most useful information from a given text\"\n",
    "response = groq_chat(user_input, system_message, vault_embeddings_tensor, vault_content, model)\n",
    "print(NEON_GREEN + \"Groq Response: \\n\\n\" + response + RESET_COLOUR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
